{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wyznaczanie keywordów przy użyciu TF-IDF #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metoda TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF (ang. TF – term frequency, IDF – inverse document frequency) – ważenie częstością termów – odwrotna częstość w dokumentach – jedna z metod obliczania wagi słów w oparciu o liczbę ich wystąpień, należąca do grupy algorytmów obliczających statystyczne wagi termów. Każdy dokument reprezentowany jest przez wektor, składający się z wag słów występujących w tym dokumencie. TFIDF informuje o częstości wystąpienia termów uwzględniając jednocześnie odpowiednie wyważenie znaczenia lokalnego termu i jego znaczenia w kontekście pełnej kolekcji dokumentów. \n",
    "\n",
    "Więcej informacji można znaleźć tutaj: https://www.sunrisesystem.pl/blog/2411-czym-jest-tfidf-i-jak-wplywa-na-pozycjonowanie-strony-ster-n.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zbiór treningowy i testowy pochodzi ze strony Stack Overflow. Zgromadzone dokumenty/artykuły/notatki dotyczą głównie tematów dotyczących technologii, programowania i ogólnie pojętej informatyki wraz z naukami pokrewnymi. Są one w języku angielskim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotowany algorytm dobrze spisuje się dla artykułów o tematyce technologicznej, ale dobrze radzi sobie również z innymi typami artykułów - dot. politykii, przyrody, zdrowia. Jednak muszą to być artykuły w języku angielskim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import bibliotek ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "\n",
      " accepted_answer_id          float64\n",
      "answer_count                  int64\n",
      "body                         object\n",
      "comment_count                 int64\n",
      "community_owned_date         object\n",
      "creation_date                object\n",
      "favorite_count              float64\n",
      "id                            int64\n",
      "last_activity_date           object\n",
      "last_edit_date               object\n",
      "last_editor_display_name     object\n",
      "last_editor_user_id         float64\n",
      "owner_display_name           object\n",
      "owner_user_id               float64\n",
      "post_type_id                  int64\n",
      "score                         int64\n",
      "tags                         object\n",
      "title                        object\n",
      "view_count                    int64\n",
      "dtype: object\n",
      "Number of questions,columns= (20000, 19)\n"
     ]
    }
   ],
   "source": [
    "# zbiór treningowy\n",
    "df_idf = pd.read_json(\"stackoverflow-data-idf.json\",lines=True)\n",
    " \n",
    "# dane o formacie danych zbioru treningowego\n",
    "print(\"Schema:\\n\\n\",df_idf.dtypes)\n",
    "print(\"Number of questions,columns=\",df_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing danych tekstowych - sprowadzanie do lowercase'a, usuwanie tagów, znaków specjalnych i cyfr\n",
    "def pre_process(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"<!--?.*?-->\",\"\", text)\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loop variable as parameter in asynchronous function call p i have an object with the following form p pre code sortedfilters task appointment email code pre p now i want to use a code for in code loop to build my tasks array for code async parallel code which contains the functions to be executed asynchronously p pre code var tasks for var entity in sortedfilters tasks push function callback var entityresult fetchrecordsforentity entity businessunits sortedfilters var formattedentityresults formatresults entity entityresult callback null formattedentityresults code pre p the problem is that at the moment the functions are called code entity code points at the last value of the loop in this case it would be code email code p p how can i nail the exact value at the moment the function is added to the array p '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf['text'] = df_idf['title'] + df_idf['body']\n",
    "df_idf['text'] = df_idf['text'].apply(lambda x:pre_process(x))\n",
    " \n",
    "# przykładowy tekst ze zbioru treningowego\n",
    "df_idf['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/anaconda3/envs/Studies/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)\n",
    "\n",
    "# wczytujemy zbiór stop-words - lista słów do odrzucenia\n",
    "stopwords = get_stop_words(\"stopwords.txt\")\n",
    " \n",
    "# zbiór dokumentów\n",
    "docs = df_idf['text'].tolist()\n",
    "  \n",
    "# ignorujemy wyrazy, które znajdują się w >= 85% wszystkich dokumentów \n",
    "# wyrzucamy stop words\n",
    "cv = CountVectorizer(max_df=0.85, stop_words=stopwords)\n",
    "word_count_vector = cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pawel/anaconda3/envs/Studies/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=10000)\n",
    "word_count_vector = cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['serializing',\n",
       " 'private',\n",
       " 'struct',\n",
       " 'public',\n",
       " 'class',\n",
       " 'contains',\n",
       " 'properties',\n",
       " 'string',\n",
       " 'serialize',\n",
       " 'attempt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wczytujemy zbiór testowy, wyciągamy treści artykułów i ich tytuły\n",
    "df_test = pd.read_json(\"stackoverflow-test.json\", lines=True)\n",
    "df_test['text'] = df_test['title'] + df_test['body']\n",
    "df_test['text'] = df_test['text'].apply(lambda x:pre_process(x))\n",
    " \n",
    "# tworzymy zbiór testowych dokumentów\n",
    "docs_test = df_test['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    # bierzemy tylko topn najlepszych wyników\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # indeks wyrazu i tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keywords(doc):\n",
    "    # dla dokumentu doc wygeneruj tf-idf\n",
    "    tf_idf_vector = tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "    # posortuj kandydatów na keywordy zgodnie z tf-idf score\n",
    "    sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    # bierzemy 10 najlepszych wyników\n",
    "    keywords = extract_topn_from_vector(feature_names, sorted_items, 10)\n",
    "\n",
    "    # wypisywanie wyniku\n",
    "    print(\"\\n=====Doc=====\")\n",
    "    print(doc)\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        print(k, keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykłady działania ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      "\n",
      "Researchers have come up with a method for creating realistic-looking — but fake — \n",
      "videos of anyone by using just a single image of them with a trained artificial \n",
      "intelligence system. It's a potentially worrisome capability in the runup to the \n",
      "2020 United States presidential election, as falsified videos of candidates are \n",
      "expected to spread. Researchers at the Samsung AI Center in Moscow and the Moscow-based \n",
      "Skolkovo Institute of Science and Technology explained the feat in a paper published \n",
      "this week to the arXiv, an online academic pre-print service. They said they were able \n",
      "to animate one or several photos of people by first training an AI system on a dataset \n",
      "of videos including many celebrities, so it could learn about key points on the face. \n",
      "After that, the AI system was able to combine that familiarity with one or more images \n",
      "of a person to come up with a convincing \"talking head\"-style video of them. \n",
      "\n",
      "\n",
      "===Keywords===\n",
      "ai 0.451\n",
      "videos 0.37\n",
      "system 0.199\n",
      "come 0.194\n",
      "united 0.159\n",
      "trained 0.159\n",
      "capability 0.159\n",
      "science 0.15\n",
      "spread 0.148\n",
      "fake 0.144\n"
     ]
    }
   ],
   "source": [
    "# podajemy dokument, dla którego chcemy przeprowadzić ekstrakcję słów kluczowych\n",
    "doc1 = '''\n",
    "Researchers have come up with a method for creating realistic-looking — but fake — \n",
    "videos of anyone by using just a single image of them with a trained artificial \n",
    "intelligence system. It's a potentially worrisome capability in the runup to the \n",
    "2020 United States presidential election, as falsified videos of candidates are \n",
    "expected to spread. Researchers at the Samsung AI Center in Moscow and the Moscow-based \n",
    "Skolkovo Institute of Science and Technology explained the feat in a paper published \n",
    "this week to the arXiv, an online academic pre-print service. They said they were able \n",
    "to animate one or several photos of people by first training an AI system on a dataset \n",
    "of videos including many celebrities, so it could learn about key points on the face. \n",
    "After that, the AI system was able to combine that familiarity with one or more images \n",
    "of a person to come up with a convincing \"talking head\"-style video of them. \n",
    "'''\n",
    "\n",
    "find_keywords(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      "\n",
      "Computer scientists at The University of Texas at Austin have taught an \n",
      "artificial intelligence agent how to do something that usually only humans \n",
      "can do -- take a few quick glimpses around and infer its whole environment, \n",
      "a skill necessary for the development of effective search-and-rescue robots \n",
      "that one day can improve the effectiveness of dangerous missions. The team, \n",
      "led by professor Kristen Grauman, Ph.D. candidate Santhosh Ramakrishnan and \n",
      "former Ph.D. candidate Dinesh Jayaraman (now at the University of California, \n",
      "Berkeley) published their results today in the journal Science Robotics.\n",
      "Most AI agents -- computer systems that could endow robots or other machines \n",
      "with intelligence -- are trained for very specific tasks -- such as to \n",
      "recognize an object or estimate its volume -- in an environment they have \n",
      "experienced before, like a factory. But the agent developed by Grauman and \n",
      "Ramakrishnan is general purpose, gathering visual information that can then \n",
      "be used for a wide range of tasks.\n",
      "\n",
      "\n",
      "===Keywords===\n",
      "ph 0.303\n",
      "robots 0.29\n",
      "candidate 0.287\n",
      "university 0.283\n",
      "agent 0.22\n",
      "tasks 0.198\n",
      "computer 0.198\n",
      "environment 0.176\n",
      "berkeley 0.156\n",
      "journal 0.15\n"
     ]
    }
   ],
   "source": [
    "doc2 = '''\n",
    "Computer scientists at The University of Texas at Austin have taught an \n",
    "artificial intelligence agent how to do something that usually only humans \n",
    "can do -- take a few quick glimpses around and infer its whole environment, \n",
    "a skill necessary for the development of effective search-and-rescue robots \n",
    "that one day can improve the effectiveness of dangerous missions. The team, \n",
    "led by professor Kristen Grauman, Ph.D. candidate Santhosh Ramakrishnan and \n",
    "former Ph.D. candidate Dinesh Jayaraman (now at the University of California, \n",
    "Berkeley) published their results today in the journal Science Robotics.\n",
    "Most AI agents -- computer systems that could endow robots or other machines \n",
    "with intelligence -- are trained for very specific tasks -- such as to \n",
    "recognize an object or estimate its volume -- in an environment they have \n",
    "experienced before, like a factory. But the agent developed by Grauman and \n",
    "Ramakrishnan is general purpose, gathering visual information that can then \n",
    "be used for a wide range of tasks.\n",
    "'''\n",
    "\n",
    "find_keywords(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      "\n",
      "A fleet of driverless cars working together to keep traffic moving smoothly can improve \n",
      "overall traffic flow by at least 35 percent, researchers have shown.\n",
      "The researchers, from the University of Cambridge, programmed a small fleet of \n",
      "miniature robotic cars to drive on a multi-lane track and observed how the traffic \n",
      "flow changed when one of the cars stopped. When the cars were not driving cooperatively, \n",
      "any cars behind the stopped car had to stop or slow down and wait for a gap in the traffic, \n",
      "as would typically happen on a real road. A queue quickly formed behind the stopped car \n",
      "and overall traffic flow was slowed. However, when the cars were communicating with each \n",
      "other and driving cooperatively, as soon as one car stopped in the inner lane, it sent a \n",
      "signal to all the other cars. Cars in the outer lane that were in immediate proximity of \n",
      "the stopped car slowed down slightly so that cars in the inner lane were able to quickly \n",
      "pass the stopped car without having to stop or slow down significantly.\n",
      "\n",
      "\n",
      "===Keywords===\n",
      "cars 0.693\n",
      "traffic 0.372\n",
      "car 0.351\n",
      "flow 0.199\n",
      "driving 0.153\n",
      "down 0.139\n",
      "quickly 0.133\n",
      "slow 0.115\n",
      "inner 0.114\n",
      "proximity 0.096\n"
     ]
    }
   ],
   "source": [
    "doc3 = '''\n",
    "A fleet of driverless cars working together to keep traffic moving smoothly can improve \n",
    "overall traffic flow by at least 35 percent, researchers have shown.\n",
    "The researchers, from the University of Cambridge, programmed a small fleet of \n",
    "miniature robotic cars to drive on a multi-lane track and observed how the traffic \n",
    "flow changed when one of the cars stopped. When the cars were not driving cooperatively, \n",
    "any cars behind the stopped car had to stop or slow down and wait for a gap in the traffic, \n",
    "as would typically happen on a real road. A queue quickly formed behind the stopped car \n",
    "and overall traffic flow was slowed. However, when the cars were communicating with each \n",
    "other and driving cooperatively, as soon as one car stopped in the inner lane, it sent a \n",
    "signal to all the other cars. Cars in the outer lane that were in immediate proximity of \n",
    "the stopped car slowed down slightly so that cars in the inner lane were able to quickly \n",
    "pass the stopped car without having to stop or slow down significantly.\n",
    "'''\n",
    "\n",
    "find_keywords(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      "\n",
      "McMaster researchers have developed a simple and highly novel form of computing \n",
      "by shining patterned bands of light and shadow through different facets of a \n",
      "polymer cube and reading the combined results that emerge.\n",
      "The material in the cube reads and reacts intuitively to the light in \n",
      "much the same way a plant would turn to the sun, or a cuttlefish would \n",
      "change the color of its skin. The researchers are graduate students in \n",
      "chemistry supervised by Kalaichelvi Saravanamuttu, an associate professor of \n",
      "chemistry and chemical biology whose lab focuses on ideas inspired by natural \n",
      "biological systems. The researchers were able to use their new process to \n",
      "perform simple addition and subtraction questions.\n",
      "\n",
      "\n",
      "===Keywords===\n",
      "cube 0.389\n",
      "light 0.301\n",
      "polymer 0.216\n",
      "plant 0.213\n",
      "professor 0.211\n",
      "skin 0.201\n",
      "computing 0.197\n",
      "lab 0.191\n",
      "natural 0.188\n",
      "simple 0.184\n"
     ]
    }
   ],
   "source": [
    "doc4 = '''\n",
    "McMaster researchers have developed a simple and highly novel form of computing \n",
    "by shining patterned bands of light and shadow through different facets of a \n",
    "polymer cube and reading the combined results that emerge.\n",
    "The material in the cube reads and reacts intuitively to the light in \n",
    "much the same way a plant would turn to the sun, or a cuttlefish would \n",
    "change the color of its skin. The researchers are graduate students in \n",
    "chemistry supervised by Kalaichelvi Saravanamuttu, an associate professor of \n",
    "chemistry and chemical biology whose lab focuses on ideas inspired by natural \n",
    "biological systems. The researchers were able to use their new process to \n",
    "perform simple addition and subtraction questions.\n",
    "'''\n",
    "\n",
    "find_keywords(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artykuły o innej tematyce niz technika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      "\n",
      "With eight million tons of plastic trash added to our oceans every year, \n",
      "the seas that we depend on for survival are in grave danger. Besides \n",
      "washing up on shorelines, plastic is now found just about everywhere and \n",
      "in everything. It’s in the deepest ocean trenches, on the most remote \n",
      "shorelines, in all levels of the marine food web, in the water we drink, \n",
      "the food we eat, and even the air we breathe. adidas, in partnership with \n",
      "Parley for the Oceans, is trying to do something about it.\n",
      "Now in its third year, the annual Run For The Oceans, an initiative \n",
      "established and driven by adidas in collaboration with Parley, has helped \n",
      "the development of the Parley Ocean School Program. In 2018 alone, the run \n",
      "united nearly a million runners around the world and raised $1 million, \n",
      "with adidas contributing one dollar for every kilometer run directly to the \n",
      "program. This year the run will be bigger and, given the urgency of the marine\n",
      "plastic pollution crisis, more important than ever.\n",
      "\n",
      "\n",
      "===Keywords===\n",
      "million 0.433\n",
      "year 0.334\n",
      "food 0.31\n",
      "run 0.259\n",
      "trash 0.172\n",
      "annual 0.172\n",
      "united 0.168\n",
      "water 0.165\n",
      "tons 0.165\n",
      "program 0.161\n"
     ]
    }
   ],
   "source": [
    "doc5 = '''\n",
    "With eight million tons of plastic trash added to our oceans every year, \n",
    "the seas that we depend on for survival are in grave danger. Besides \n",
    "washing up on shorelines, plastic is now found just about everywhere and \n",
    "in everything. It’s in the deepest ocean trenches, on the most remote \n",
    "shorelines, in all levels of the marine food web, in the water we drink, \n",
    "the food we eat, and even the air we breathe. adidas, in partnership with \n",
    "Parley for the Oceans, is trying to do something about it.\n",
    "Now in its third year, the annual Run For The Oceans, an initiative \n",
    "established and driven by adidas in collaboration with Parley, has helped \n",
    "the development of the Parley Ocean School Program. In 2018 alone, the run \n",
    "united nearly a million runners around the world and raised $1 million, \n",
    "with adidas contributing one dollar for every kilometer run directly to the \n",
    "program. This year the run will be bigger and, given the urgency of the marine\n",
    "plastic pollution crisis, more important than ever.\n",
    "'''\n",
    "\n",
    "find_keywords(doc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      "\n",
      "Ian Lavery, the Labour party chairman, has hit out at a section of pro-remain \n",
      "campaigners for sneering at “ordinary people” with pro-Brexit views and sniping \n",
      "at those who want to see the results of the 2016 poll respected.\n",
      "As Jeremy Corbyn faces intense pressure to back a “people’s vote” in the wake \n",
      "of the European elections, Lavery argued in an article for the Guardian that \n",
      "Labour would not win a general election “simply by fighting for the biggest share \n",
      "of the 48%”. He said both sides needed to come together to fight the prospect of \n",
      "a no-deal Brexit being pushed by some of the Conservative leadership candidates \n",
      "who are competing to be the next prime minister.\n",
      "\n",
      "===Keywords===\n",
      "who 0.291\n",
      "people 0.289\n",
      "wake 0.232\n",
      "pressure 0.229\n",
      "ordinary 0.224\n",
      "biggest 0.224\n",
      "vote 0.221\n",
      "poll 0.217\n",
      "prime 0.209\n",
      "sides 0.205\n"
     ]
    }
   ],
   "source": [
    "doc6 = '''\n",
    "Ian Lavery, the Labour party chairman, has hit out at a section of pro-remain \n",
    "campaigners for sneering at “ordinary people” with pro-Brexit views and sniping \n",
    "at those who want to see the results of the 2016 poll respected.\n",
    "As Jeremy Corbyn faces intense pressure to back a “people’s vote” in the wake \n",
    "of the European elections, Lavery argued in an article for the Guardian that \n",
    "Labour would not win a general election “simply by fighting for the biggest share \n",
    "of the 48%”. He said both sides needed to come together to fight the prospect of \n",
    "a no-deal Brexit being pushed by some of the Conservative leadership candidates \n",
    "who are competing to be the next prime minister.'''\n",
    "\n",
    "find_keywords(doc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      "\n",
      "A recent study examines the health impact of consuming alcohol at different ages. \n",
      "The authors conclude that, for people over the age of 50, the health risks may \n",
      "be less severe. Heavy drinking is linked to a range of serious health consequences.\n",
      "These include certain cancers, liver and heart disease, and damage to the nervous \n",
      "system, including the brain. However, as has been exhaustively covered in the \n",
      "popular press, drinking in moderation might have certain health benefits.\n",
      "A number of studies have concluded that drinking alcohol at a low level could \n",
      "have a protective effect.\n",
      "\n",
      "\n",
      "===Keywords===\n",
      "health 0.666\n",
      "risks 0.195\n",
      "ages 0.18\n",
      "damage 0.178\n",
      "brain 0.176\n",
      "study 0.172\n",
      "covered 0.172\n",
      "benefits 0.172\n",
      "severe 0.169\n",
      "authors 0.169\n"
     ]
    }
   ],
   "source": [
    "doc7 = '''\n",
    "A recent study examines the health impact of consuming alcohol at different ages. \n",
    "The authors conclude that, for people over the age of 50, the health risks may \n",
    "be less severe. Heavy drinking is linked to a range of serious health consequences.\n",
    "These include certain cancers, liver and heart disease, and damage to the nervous \n",
    "system, including the brain. However, as has been exhaustively covered in the \n",
    "popular press, drinking in moderation might have certain health benefits.\n",
    "A number of studies have concluded that drinking alcohol at a low level could \n",
    "have a protective effect.\n",
    "'''\n",
    "\n",
    "find_keywords(doc7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polskie artykuły"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      "\n",
      "Gdyby szukać analogii w świecie przyrody, to hakerów bez problemu porównać \n",
      "można by było do… mrówek. Potrafią przecisnąć się nawet przez najmniejszą \n",
      "możliwą szczelinę. Gdy tylko dostępna jest okazja, z pewnością ją wykorzystają. \n",
      "Takimi szczelinami, w naszych komputerach są właśnie luki w oprogramowaniach, \n",
      "które otwierają drogę do danych i informacji przechowywanych na dyskach.\n",
      "Czym właściwie jest luka? To swego rodzaju „furtka” dla hakerów, umożliwiająca \n",
      "zainfekowanie komputera złośliwym oprogramowaniem. Mówi się o niej, jako o błędzie \n",
      "w kodzie w zabezpieczenia aplikacji. Chociaż definicji jest wiele, wszystkie \n",
      "spójnie wyjaśniają, że luka umożliwia hakerowi wykonywanie czynności na naszym \n",
      "komputerze, uzyskuje on zatem dostęp do danych, podszywając się pod inną jednostkę/program.\n",
      "\n",
      "\n",
      "===Keywords===\n",
      "na 0.831\n",
      "pod 0.495\n",
      "program 0.253\n"
     ]
    }
   ],
   "source": [
    "doc8 = '''\n",
    "Gdyby szukać analogii w świecie przyrody, to hakerów bez problemu porównać \n",
    "można by było do… mrówek. Potrafią przecisnąć się nawet przez najmniejszą \n",
    "możliwą szczelinę. Gdy tylko dostępna jest okazja, z pewnością ją wykorzystają. \n",
    "Takimi szczelinami, w naszych komputerach są właśnie luki w oprogramowaniach, \n",
    "które otwierają drogę do danych i informacji przechowywanych na dyskach.\n",
    "Czym właściwie jest luka? To swego rodzaju „furtka” dla hakerów, umożliwiająca \n",
    "zainfekowanie komputera złośliwym oprogramowaniem. Mówi się o niej, jako o błędzie \n",
    "w kodzie w zabezpieczenia aplikacji. Chociaż definicji jest wiele, wszystkie \n",
    "spójnie wyjaśniają, że luka umożliwia hakerowi wykonywanie czynności na naszym \n",
    "komputerze, uzyskuje on zatem dostęp do danych, podszywając się pod inną jednostkę/program.\n",
    "'''\n",
    "\n",
    "find_keywords(doc8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Doc=====\n",
      "\n",
      "Czym jest olej kokosowy?\n",
      "Olej kokosowy otrzymuje się przez rozgrzanie i tłoczenie miąższu kokosa. \n",
      "Już od wieków stosuje się go nie tylko w kuchni, ale i w codziennej pielęgnacji. \n",
      "Najwięcej używają go Azjatki, przede wszystkim popularny jest w Indiach i \n",
      "na Półwyspie Indyjskim. Stosowany na skórę pomoże w walce z rozstępami oraz \n",
      "cellulitem, można go też stosować do nawilżania skórek przy paznokciach. \n",
      "Sprawdzi się równie świetnie jako baza do maseczek i peelingów do twarzy i ust. \n",
      "Olej kokosowy jest w stanie stałym, wystarczy jednak lekko go podgrzać w dłoni, \n",
      "aby stał się płynny i przez to się nakładał.\n",
      "\n",
      "\n",
      "===Keywords===\n",
      "na 0.844\n",
      "od 0.536\n"
     ]
    }
   ],
   "source": [
    "doc9 = '''\n",
    "Czym jest olej kokosowy?\n",
    "Olej kokosowy otrzymuje się przez rozgrzanie i tłoczenie miąższu kokosa. \n",
    "Już od wieków stosuje się go nie tylko w kuchni, ale i w codziennej pielęgnacji. \n",
    "Najwięcej używają go Azjatki, przede wszystkim popularny jest w Indiach i \n",
    "na Półwyspie Indyjskim. Stosowany na skórę pomoże w walce z rozstępami oraz \n",
    "cellulitem, można go też stosować do nawilżania skórek przy paznokciach. \n",
    "Sprawdzi się równie świetnie jako baza do maseczek i peelingów do twarzy i ust. \n",
    "Olej kokosowy jest w stanie stałym, wystarczy jednak lekko go podgrzać w dłoni, \n",
    "aby stał się płynny i przez to się nakładał.\n",
    "'''\n",
    "\n",
    "find_keywords(doc9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla artykułów w języku polskim nasz algorytm niestety zawodzi, problem ten można rozwiązać przygotowując zbiór treningowy z artykułami w języku polskim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
